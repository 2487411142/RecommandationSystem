# -*- coding: utf-8 -*-
"""New_ETL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WMWHMVocXsJS5LQLQUZeC6iz2fn4bTKf

# setting enviroment
"""

!sudo apt update
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz
!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz
!tar xf spark-3.2.1-bin-hadoop3.2.tgz
!pip install -q findspark
!pip install pyspark
!pip install py4j

import os
import sys
# os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
# os.environ["SPARK_HOME"] = "/content/spark-3.2.1-bin-hadoop3.2"


import findspark
findspark.init()
findspark.find()

import pyspark

from pyspark.sql import DataFrame, SparkSession
from typing import List
import pyspark.sql.types as T
import pyspark.sql.functions as F

spark= SparkSession \
       .builder \
       .appName("Our First Spark Example") \
       .getOrCreate()

spark

spark

!pip install kaggle


from google.colab import files
!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json


!kaggle datasets download -d mkechinov/ecommerce-purchase-history-from-electronics-store


!unzip ecommerce-purchase-history-from-electronics-store.zip -d ecommerce_dataset

!ls ecommerce_dataset

# Step 1: Install kagglehub
!pip install kagglehub

# Step 2: Import kagglehub and download dataset
import kagglehub

path = kagglehub.dataset_download("mkechinov/ecommerce-purchase-history-from-electronics-store")
print("Path to dataset files:", path)

"""#Spark Part to read data and prepare"""

import sys
from pyspark.sql import SparkSession, functions, types
from pyspark.ml.feature import StringIndexer
from pyspark.sql.functions import current_date, year
from pyspark.sql.functions import to_timestamp

"""#Data Schema"""

transactions_schema = types.StructType([
    types.StructField('event_time', types.TimestampType()),
    types.StructField('order_id', types.StringType()),
    types.StructField('product_id', types.StringType()),
    types.StructField('category_id', types.StringType()),
    types.StructField('category_code', types.StringType()),
    types.StructField('brand', types.StringType()),
    types.StructField('price', types.FloatType()),
    types.StructField('user_id', types.StringType())
])

"""# import data"""

transactions = spark.read.csv(path + "/kz.csv", schema=transactions_schema, header=True)

origin_num_rows = transactions.count()
print(f"Number of rows in transactions: {origin_num_rows}")

transactions.show(10, truncate=False)

"""# Counting the number of product_id, category_id, category_code, brand

"""

product_count = transactions.select("product_id").distinct().count()
category_id_count = transactions.select("category_id").distinct().count()
category_code_count = transactions.select("category_code").distinct().count()
brand_count = transactions.select("brand").distinct().count()

print(f"Unique product_id count: {product_count}")
print(f"Unique category_id count: {category_id_count}")
print(f"Unique category_code count: {category_code_count}")
print(f"Unique brand count: {brand_count}")

"""# Counting Missing value and deal with it"""

transactions = transactions.distinct()

missing_counts = transactions.select([F.sum(F.col(c).isNull().cast("int")).alias(c) for c in transactions.columns])

missing_counts.show()

new_num_rows = transactions.count()
print(f"Number of rows in transactions: {new_num_rows}")

diff = origin_num_rows - new_num_rows
print(f"Reapted transactions: {diff}")

"""
# Finding the relationship bewteen category_id, category_code"""

category_mapping = (
    transactions
    .select("category_id", "category_code")
    .filter(F.col("category_code").isNotNull())
    .dropDuplicates()
)

"""# Approach 1, removing user_id is null"""

df = transactions.filter(transactions['user_id'].isNotNull())

df.show()

df_rows = df.count()
print(f"Number of rows in transactions: {df_rows}")

df_miss_counts = df.select([F.sum(F.col(c).isNull().cast("int")).alias(c) for c in df.columns])

df_miss_counts.show()

filled_code = (
     df
    .join(category_mapping.withColumnRenamed("category_code", "category_code_mapping"), on="category_id", how="left")
    .withColumn(
        "category_code",
        F.coalesce(F.col("category_code_mapping"), F.lit("na"))
    )
    .drop("category_code_mapping")
)

miss_filled_code_cnt = filled_code.select([F.sum(F.col(c).isNull().cast("int")).alias(c) for c in filled_code.columns])

miss_filled_code_cnt.show()

na_count = filled_code.filter(filled_code['category_code'] == 'na').count()

print(f"Number of 'na' in category_code: {na_count}")

transactions_cleaned = filled_code.filter(F.col("category_code") != "na")

transactions_cleaned_row = transactions_cleaned.count()
print(f"Number of rows in transactions: {transactions_cleaned_row}")

brand_mapping = (
    transactions_cleaned.groupBy("product_id", "price")
    .agg(F.first("brand", ignorenulls=True).alias("brand_mapping"))
)

filled_brand = (
    transactions_cleaned
    .join(
        brand_mapping.withColumnRenamed("brand_mapping", "brand_mapping_temp"),
        on=["product_id", "price"],
        how="left"
    )
    .withColumn(
        "brand",
        F.coalesce(F.col("brand"), F.col("brand_mapping_temp"), F.lit("na"))
    )
    .drop("brand_mapping_temp")
)

filled_brand.show()

filled_brand_miss_cnts = filled_brand.select([F.sum(F.col(c).isNull().cast("int")).alias(c) for c in filled_brand.columns])

filled_brand_miss_cnts.show()

final_data = filled_brand.filter(F.col("brand") != "na")

final_data_row = final_data.count()
print(f"Number of rows in transactions: {final_data_row}")

print(filled_brand.count())

output_path = "/content/filled_brand.csv"

filled_brand.coalesce(1).write.csv(output_path, header=True, mode="overwrite")
print(f"successful : {output_path}")















